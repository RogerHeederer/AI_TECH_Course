{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MultiHeadAttention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN4nNn3GWI8wba38EjmrsVv"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_PATVTRWKTlB","executionInfo":{"status":"ok","timestamp":1612687745052,"user_tz":-540,"elapsed":1707,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VKyoFOIZOAm3"},"source":["#Scaled Dot-product Attention = self-attention"]},{"cell_type":"markdown","metadata":{"id":"ZIG49xzHbhb0"},"source":["### Scaled Dot-Product Attention (SDPA)\n","- Data $X \\in \\mathbb{R}^{n \\times d}$ where $n$ is the number data and $d$ is the data dimension\n","- Query and Key $Q, K \\in \\mathbb{R}^{n \\times d_K}$ \n","- Value $V \\in \\mathbb{R}^{n \\times d_V} $\n","\n","$\\text{Attention}(Q,K,V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_K}} \\right)V \\in \\mathbb{R}^{n \\times d_V} $"]},{"cell_type":"markdown","metadata":{"id":"6B87yWG0bmXv"},"source":["Q,K,V는 처음 데이터 X의 임베딩 값과 같다"]},{"cell_type":"code","metadata":{"id":"0CVwX-J9N4l3","executionInfo":{"status":"ok","timestamp":1612687745396,"user_tz":-540,"elapsed":2045,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["class ScaledDotProductAttention(nn.Module):\n","    def forward(self,Q,K,V,mask=None):\n","        d_K = K.size()[-1] #Key dimension을 넣어준다. 128\n","        scores = Q.matmul(K.transpose(-2,-1)) / np.sqrt(d_K)\n","        if mask is not None:\n","            scores = scores.masked_fill(mask==0, -1e9)#마스크 값이 0이면 극음수로 바꿔줌. 소프트맥스 계산시에 반영 안되게 하기 위해\n","        attention = F.softmax(scores,dim=-1)\n","        out = attention.matmul(V)\n","        return out, attention"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"u93Q946DcmQc","executionInfo":{"status":"ok","timestamp":1612687745397,"user_tz":-540,"elapsed":2042,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["SPDA = ScaledDotProductAttention()\n","n_batch,d_K,d_V = 3, 128, 256 #K(Q)의 차원이 V의 차원과 같을 필요는 없다.\n","n_Q, n_K, n_V = 30,50,50\n","#랜덤 숫자로 Q,K,V 값 만들어준다.\n","Q = torch.rand(n_batch, n_Q, d_K) #3x30x128\n","K = torch.rand(n_batch, n_K, d_K) #3x50x128\n","V = torch.rand(n_batch, n_V, d_V) #3x50x256"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dAt0zVVbcmdk","executionInfo":{"status":"ok","timestamp":1612687745398,"user_tz":-540,"elapsed":2039,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"f988897e-9a64-4be8-9f5c-55f9eee8a155"},"source":["Q.shape, K.shape, K.size()[-1]"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3, 30, 128]), torch.Size([3, 50, 128]), 128)"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"QGMh4oKccnHw","executionInfo":{"status":"ok","timestamp":1612687745398,"user_tz":-540,"elapsed":2034,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["out, attention = SPDA.forward(Q,K,V,mask=None)"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgY6TD4hdCJv","executionInfo":{"status":"ok","timestamp":1612687745399,"user_tz":-540,"elapsed":2030,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"e3a063d1-155c-433e-dde8-aedb27624827"},"source":["out.shape, attention.shape"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3, 30, 256]), torch.Size([3, 30, 50]))"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_ckok6Le2W2","executionInfo":{"status":"ok","timestamp":1612687745401,"user_tz":-540,"elapsed":2027,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"a232c749-26a8-4218-cf8f-dce6878af92e"},"source":["def sh(x): return str(x.shape)[11:-1] \n","print (\"SDPA: Q%s K%s V%s => out%s attention%s\"%\n","       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))"],"execution_count":62,"outputs":[{"output_type":"stream","text":["SDPA: Q[3, 30, 128] K[3, 50, 128] V[3, 50, 256] => out[3, 30, 256] attention[3, 30, 50]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QdZvl_mvgeOC"},"source":["멀티헤드어텐션으로 작성_ 간단히 작성한 버젼"]},{"cell_type":"code","metadata":{"id":"19vtit2Xe3Sd","executionInfo":{"status":"ok","timestamp":1612687746191,"user_tz":-540,"elapsed":2813,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":["n_batch, n_head, d_K, d_V = 3,5,128,256\n","n_Q, n_K, n_V = 30, 50, 50 #n_K와 n_V의 개수는 같아야 한다.\n","Q = torch.rand(n_batch,n_head,n_Q,d_K)\n","K = torch.rand(n_batch,n_head,n_K,d_K)\n","V = torch.rand(n_batch,n_head,n_V,d_V)\n","out,attention = SPDA.forward(Q,K,V,mask=None)"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_EQXG8Pg18h","executionInfo":{"status":"ok","timestamp":1612687746192,"user_tz":-540,"elapsed":2810,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"81943268-2d03-4e85-c666-28a719596134"},"source":["out.shape, attention.shape # 차원에 5가 추가된 것을 확인할 수 있다."],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([3, 5, 30, 256]), torch.Size([3, 5, 30, 50]))"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"l1QaPsMIhaKU"},"source":["# 멀티헤드어텐션 상세히 작성하기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130},"id":"9TH8Av3vg2zO","executionInfo":{"status":"error","timestamp":1612687835471,"user_tz":-540,"elapsed":1226,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"09e86d82-e857-4dd8-86ed-76e80c9b9bc4"},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self,d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=None):\n","        \"\"\"\n","        :param d_feat: feature dimension\n","        :param n_head: number of heads\n","        :param actv: activation after each linear layer\n","        :param USE_BIAS: whether to use bias\n","        :param dropout_p: dropout rate\n","        :device: which device to use (e.g., cuda:0)\n","        \"\"\"\n","        super(MultiHeadedAttention,self).__init__()\n","        if (d_feat%n_head) != 0:\n","            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\"%(d_feat,n_head)) \n","        self.d_feat = d_feat\n","        self.n_head = n_head\n","        self.d_head = self.d_feat // self.n_head\n","        self.actv = actv\n","        self.USE_BIAS = USE_BIAS\n","        self.dropout_p = dropout_p # prob. of zeroed\n","\n","        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","\n","        self.dropout = nn.Dropout(p=self.dropout_p)\n","    \n","    def forward(self,Q,K,V,mask=None):\n","        \"\"\"\n","        :param Q: [n_batch, n_Q, d_feat]\n","        :param K: [n_batch, n_K, d_feat]\n","        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \n","        :param mask: \n","        \"\"\"\n","        n_batch = Q.shape[0]\n","        Q_feat = self.lin_Q(Q) \n","        K_feat = self.lin_K(K) \n","        V_feat = self.lin_V(V)\n","        # Q_feat: [n_batch, n_Q, d_feat]\n","        # K_feat: [n_batch, n_K, d_feat]\n","        # V_feat: [n_batch, n_V, d_feat]\n","\n","        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\n","        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        # Q_split: [n_batch, n_head, n_Q, d_head]\n","        # K_split: [n_batch, n_head, n_K, d_head]\n","        # V_split: [n_batch, n_head, n_V, d_head]\n","\n","        # Multi-Headed Attention\n","        d_K = K.size()[-1] # key dimension\n","        scores = torch.matmul(Q_split, K_split.permute())\n","        if mask is not None:\n","            scores = scores.masked_fill(mask==0,-1e9)\n","        attention = torch.softmax(scores,dim=-1)\n","        x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper\n","        # attention: [n_batch, n_head, n_Q, n_K]\n","        # x_raw: [n_batch, n_head, n_Q, d_head]\n","\n","        # Reshape x\n","        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n","        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n","        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n","        # x_rsh2: [n_batch, n_Q, d_feat]\n","\n","        # Linear\n","        x = self.lin_O(x_rsh2)\n","        # x: [n_batch, n_Q, d_feat]\n","        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n","               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n","               'scores':scores,'attention':attention,\n","               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\n","        return out"],"execution_count":67,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-67-99d53e5cd774>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    scores = # FILL IN HERE\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"UGXrQXg9n5LL","executionInfo":{"status":"error","timestamp":1612687746194,"user_tz":-540,"elapsed":2805,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}},"outputId":"5b4155d8-0d66-4ccf-f94f-0ac08f0b9208"},"source":["# Self-Attention Layer\n","n_batch = 128\n","n_src   = 32\n","d_feat  = 200\n","n_head  = 5\n","src = torch.rand(n_batch,n_src,d_feat)\n","self_attention = MultiHeadedAttention(d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device)\n","out = self_attention.forward(src,src,src,mask=None)\n","\n","Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\n","Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\n","scores,attention = out['scores'],out['attention']\n","x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\n","\n","# Print out shapes\n","def sh(_x): return str(_x.shape)[11:-1] \n","print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\n","print ()\n","print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\n","print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\n","print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\n","print ()\n","print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\n","print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\n","print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\n","print ()\n","print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\n","print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\n","print ()\n","print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\n","print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\n","print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\n","print ()\n","print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))\n"],"execution_count":66,"outputs":[{"output_type":"error","ename":"ModuleAttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-fd6f477bd701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_src\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mself_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiHeadedAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mUSE_BIAS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mQ_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK_feat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q_feat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'K_feat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'V_feat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-65-96f0b99e0a87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mV_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_V\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mQ_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_haed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mK_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mV_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_head\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleAttributeError\u001b[0m: 'MultiHeadedAttention' object has no attribute 'd_haed'"]}]},{"cell_type":"code","metadata":{"id":"RMMgI-F6oXUO","executionInfo":{"status":"aborted","timestamp":1612687746193,"user_tz":-540,"elapsed":2801,"user":{"displayName":"Heederer Roger","photoUrl":"","userId":"05540873787818726760"}}},"source":[""],"execution_count":null,"outputs":[]}]}