{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"학습일지_2주_3일차.ipynb","provenance":[],"authorship_tag":"ABX9TyPyvi5lm9SxA17KOKG6jzmm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kwj7btIM5X68"},"source":["#활성함수 ( activation function )\n","\n","실수 공간에서 정의된 비선형 함수로서 딥러닝에서 매우 중요한 개념\n","\n","활성함수를 안쓰면 딥러닝은 선형 모델과 차이가 없다.\n","\n","시그모이드 tanh 함수 등이 전통적으로 많이 쓰이던 활성화 함수이지만\n","\n","딥러닝에서는 ReLU 를 많이 쓴다."]},{"cell_type":"markdown","metadata":{"id":"KWbhtyam5-X5"},"source":["#신경망 모델 = 선형 모델을 활성함수로 합성한 함수"]},{"cell_type":"markdown","metadata":{"id":"WXUF0Yk86_fd"},"source":["#Forward propagation (순전파)\n","\n","이건 학습이 아니고, 주어진 인풋이 들어오고, 레이어들을 거쳐 아웃풋이 나오는 과정을 의미하는 것.\n","\n","백프로파게이션이 학습하는 과정이다"]},{"cell_type":"markdown","metadata":{"id":"3VCugmPq7lar"},"source":["# 레이어(층)이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적인 학습이 가능.\n","\n","- 층이 깊으면, 적은 파라미터들로 복잡한 테스크를 처리할 수 있는 모델을 구현할 수 있다.\n","- 층이 얇으면, 필요한 파라미터(뉴런)의 숫자가 기하급수적으로 늘어나서 넓은 신경망이 되어야 한다.\n","\n","* 층이 깊으면, 복잡한 목적함수를 상대적으로 빨리 근사시킬 수 있지만, 최적화가 더 쉽다고 얘기할 순 없다. 즉 층이 깊어질수록 학습시키기에 더 복잡해진다는 얘기\n","레이어(층)이 얇으면 "]},{"cell_type":"markdown","metadata":{"id":"bDXBkuKl9TU0"},"source":["# Back Propagation(역전파) 알고리즘\n","\n","합성함수를 미분하는 방법인 연쇄법칙(체인 룰)을 기반으로 자동 미분을 사용하는 알고리즘을 의미한다\n","\n","아래의 예시에서 알 수 있듯이, x에 대해 미분을 할때에도 y의 값이 필요하기 때문에, ***순전파보다 더 많은 메모리가 필요하다***"]},{"cell_type":"markdown","metadata":{"id":"W8WDVYnD-DIg"},"source":["<img src = \"https://drive.google.com/uc?export=view&id=1n2ghOrH93LyLdxisDHv3RpqaMmHIzyis\" align=left height=\"400\">"]},{"cell_type":"code","metadata":{"id":"hkeC4T-k5IAO"},"source":[""],"execution_count":null,"outputs":[]}]}